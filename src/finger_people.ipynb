{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finger People experimental playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import urllib\n",
    "import time\n",
    "from enum import Enum\n",
    "import subprocess\n",
    "import keras.models\n",
    "import pickle\n",
    "\n",
    "from helpers import imshow\n",
    "from capture import Capture, CapType\n",
    "from segmenter import *\n",
    "from gestures import *\n",
    "from game_input import GameInput\n",
    "import vision\n",
    "import dataset\n",
    "import learn\n",
    "from cnn_input import CnnInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IP Webcam\n",
    "cap_source = 'http://192.168.0.124:8080/video'\n",
    "cap_type = CapType.VIDEO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Data\n",
    "Record dense data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_name = 'run'\n",
    "max_frames = 600\n",
    "n_frames = 0\n",
    "output_dir = '../data/'\n",
    "\n",
    "output_path = os.path.join(output_dir,\n",
    "        '{}-{}.avi'.format(gesture_name, str(time.time())))\n",
    "\n",
    "cap = Capture(cap_source, cap_type)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_path, fourcc, 30.0, (160,120))\n",
    "\n",
    "while cap.is_opened() and n_frames < max_frames:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "        \n",
    "    out.write(frame)\n",
    "    n_frames += 1\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    key = cv2.waitKey(3) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "            \n",
    "cap.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record sparse data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_name = 'kick'\n",
    "max_frames = 600\n",
    "n_frames = 0\n",
    "recorded_frames = set()\n",
    "output_dir = '../data/'\n",
    "output_prefix = os.path.join(output_dir,\n",
    "        '{}-{}'.format(gesture_name, str(time.time())))\n",
    "output_path = output_prefix + '.avi'\n",
    "output_pickle = output_prefix + '.pickle'\n",
    "\n",
    "cap = Capture(cap_source, cap_type)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_path, fourcc, 30.0, (160,120))\n",
    "flow = vision.OpticalFlow(cap.read()[1])\n",
    "record = False\n",
    "\n",
    "while cap.is_opened() and len(recorded_frames) < max_frames:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    \n",
    "    out.write(frame)\n",
    "    flow.update(frame)\n",
    "    \n",
    "    if record:\n",
    "        recorded_frames.add(n_frames)        \n",
    "        cv2.circle(frame, (6,6), (5), (0,0,255), cv2.FILLED)\n",
    "    n_frames += 1\n",
    "        \n",
    "    cv2.imshow('frame', np.hstack((frame, flow.vis)))\n",
    "    \n",
    "    key = cv2.waitKey(3) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    if key == ord('f'):\n",
    "        record = not record\n",
    "            \n",
    "pickle.dump(recorded_frames, open(output_pickle, 'wb'))\n",
    "cap.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = ['kick', 'jump', 'stand', 'walk', 'run']\n",
    "ftype = '.png'\n",
    "data_dir = '../data/'\n",
    "\n",
    "for gesture in gestures:\n",
    "    output_dir = os.path.join(data_dir, 'preprocessed', gesture)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    name_id = 0\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename[:len(gesture)] != gesture or \\\n",
    "                os.path.splitext(filename)[1] != '.avi':\n",
    "            continue\n",
    "\n",
    "        input_file = os.path.join(data_dir, filename)\n",
    "        recorded_frames = None\n",
    "        pickle_file = '{}.pickle'.format(os.path.splitext(input_file)[0])\n",
    "        if os.path.exists(pickle_file):\n",
    "            recorded_frames = pickle.load(open(pickle_file, 'rb'))\n",
    "\n",
    "        cap = Capture(input_file, CapType.VIDEO)\n",
    "        ret, first_frame = cap.read()\n",
    "        cnn_input = CnnInput(first_frame, debug=True)\n",
    "\n",
    "        n_frames = 0\n",
    "        while cap.is_opened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            cnn_input.update(frame)\n",
    "\n",
    "            output_name = os.path.join(output_dir, str(name_id) + ftype)\n",
    "            if recorded_frames is None or n_frames in recorded_frames:\n",
    "                cv2.imwrite(output_name, cnn_input.frame)\n",
    "                name_id += 1\n",
    "            n_frames += 1\n",
    "\n",
    "            key = cv2.waitKey(2) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "name 'datagen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n  File \"/home/nwam/Projects/finger-people/repo/src/dataset.py\", line 53, in __getitem__\n    X, y = self.__data_generation(list_IDs_temp)\n  File \"/home/nwam/Projects/finger-people/repo/src/dataset.py\", line 44, in __data_generation\n    X[i,] = datagen.random_transform(cv2.imread(data_dir + ID))\nNameError: name 'datagen' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datagen' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-63b07af3a26c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/finger-people/repo/src/learn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                         callbacks=[checkpoint_callback])\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoints_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'final_model.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2190\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: name 'datagen' is not defined"
     ]
    }
   ],
   "source": [
    "learn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '3_augmented_data.hdf5'\n",
    "model_dir = 'models'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test how fast our model runs while also doing a basic sanity check for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stand': 2.0, 'walk': 12.0, 'run': 1.0, 'jump': 0.0, 'kick': 585.0}\n",
      "174.71 forward passes per second.\n"
     ]
    }
   ],
   "source": [
    "path = '../data/preprocessed/kick/{}.png'\n",
    "predictions = np.zeros(dataset.n_classes)\n",
    "start = time.time()\n",
    "n = 600\n",
    "for i in range(n):\n",
    "    frame = cv2.imread(path.format(i))\n",
    "    frame = np.expand_dims(frame, 0)\n",
    "    prediction = model.predict(frame)\n",
    "    label_id = np.argmax(prediction)\n",
    "    predictions[label_id] = predictions[label_id] + 1\n",
    "\n",
    "label_predictions = {}\n",
    "for i, prediction in enumerate(predictions):\n",
    "    label_predictions[dataset.id_to_class[i]] = predictions[i]\n",
    "print(label_predictions)\n",
    "\n",
    "rate = n / (time.time() - start)\n",
    "print('{:.2f} forward passes per second.'.format(rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finger People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = Capture(cap_source, cap_type)\n",
    "ret, first_frame = cap.read()\n",
    "cnn_input = CnnInput(first_frame)\n",
    "\n",
    "h = first_frame.shape[0]\n",
    "\n",
    "while cap.is_opened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    ''' CNN '''\n",
    "    cnn_input.update(frame)\n",
    "    cnn_input_4d = np.expand_dims(cnn_input.frame, 0)\n",
    "    prediction = model.predict(cnn_input_4d)\n",
    "    \n",
    "    class_id = np.argmax(prediction)\n",
    "    class_label = dataset.id_to_class[class_id]\n",
    "    if class_label == 'walk':\n",
    "        class_label = 'run'\n",
    "    \n",
    "    ''' OUTPUT / DEBUG '''\n",
    "    cnn_input_show = cv2.resize(cnn_input.frame, (h,h))\n",
    "    cv2.putText(frame, class_label, (2, h-3), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0))\n",
    "    cv2.imshow('frame', np.hstack((frame, cnn_input_show)))\n",
    "\n",
    "    ''' KEYBOARD INPUT '''\n",
    "    key = cv2.waitKey(2) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of emergency\n",
    "cap.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skin Clibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUE = 0\n",
    "SAT = 1\n",
    "VAL = 2\n",
    "channels = [HUE, SAT]\n",
    "\n",
    "HUE_RANGE = (0,180)\n",
    "SAT_RANGE = (0,256)\n",
    "VAL_RANGE = (0, 256)\n",
    "ranges = [*HUE_RANGE, *SAT_RANGE]\n",
    "\n",
    "num_bins = 32\n",
    "bins = [num_bins]*len(channels)\n",
    "\n",
    "sat_thresh = 16 # any saturations below will be thrown out due to instability\n",
    "sat_thresh_bin = int(sat_thresh/SAT_RANGE[1]*num_bins)\n",
    "\n",
    "# Get samples of skin\n",
    "cap = Capture(cap_source, cap_type)\n",
    "skin_samples = []\n",
    "for _ in range(5):\n",
    "    skin_samples.append(cv2.cvtColor(get_roi_sample(cap), cv2.COLOR_BGR2HSV))\n",
    "cap.kill()\n",
    "\n",
    "# Calculate histogram\n",
    "skin_hist = mean_hist(skin_samples, channels=channels, ranges=ranges, bins=bins)\n",
    "#skin_hist[:, :sat_thresh_bin] = 0\n",
    "plt.xlabel('Saturation')\n",
    "plt.ylabel('Hue')\n",
    "plt.title('Your Skin')\n",
    "plt.imshow(skin_hist)\n",
    "\n",
    "# Create function to mask skin\n",
    "mask_skin = lambda frame, thresh=1: hist_mask(frame, skin_hist, thresh=thresh, channels=channels, ranges=ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finger People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = Capture(cap_source, cap_type)\n",
    "\n",
    "hand = Hand()\n",
    "\n",
    "walks_left = True # The direction the finger person is facing in the (mirrored) frame\n",
    "if not walks_left:\n",
    "    hand.body_facing = Direction.RIGHT\n",
    "    \n",
    "input_toggle = False\n",
    "\n",
    "min_blob_size = 0\n",
    "mhi_alpha = 0.5\n",
    "\n",
    "# Take the first frame\n",
    "_ret, init_frame = cap.read()\n",
    "prvs = cv2.cvtColor(init_frame,cv2.COLOR_BGR2GRAY)\n",
    "frame1 = cv2.flip(init_frame, 1)\n",
    "\n",
    "h = init_frame.shape[0]\n",
    "w = init_frame.shape[1]\n",
    "hand.screen_width = w\n",
    "\n",
    "# For color representation of optical flow\n",
    "flow_vis = np.zeros_like(init_frame)\n",
    "flow_vis[...,1] = 255\n",
    "\n",
    "# Motion history image\n",
    "mag_hist = np.zeros((init_frame.shape[:2]), dtype=np.float32)\n",
    "ang_hist = np.zeros((init_frame.shape[:2]), dtype=np.float32)\n",
    "mhi = np.zeros_like(init_frame)\n",
    "mhi[...,1] = 255\n",
    "\n",
    "while cap.is_opened():\n",
    "    \n",
    "    # Get the next frame\n",
    "    _ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)  \n",
    "    if not _ret:\n",
    "        break\n",
    "    debug = frame.copy()\n",
    "      \n",
    "    \n",
    "    \n",
    "    ''' SKIN MASK '''\n",
    "    blurred = cv2.GaussianBlur(frame, (7,7), 0)\n",
    "    skin_mask = mask_skin(cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV), thresh=25)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    hand_mask = skin_mask\n",
    "    \n",
    "    hand_mask = cv2.morphologyEx(hand_mask, cv2.MORPH_OPEN, kernel, iterations = 1)\n",
    "    hand_mask = cv2.morphologyEx(hand_mask, cv2.MORPH_CLOSE, kernel, iterations = 2)\n",
    "    hand_contour = largest_blob(hand_mask, thresh=min_blob_size)\n",
    "    hand_mask = contour2mask(hand_contour, shape=skin_mask)\n",
    "    \n",
    "    hand.position = contour_pos(hand_contour)\n",
    "    \n",
    "    # debug\n",
    "    if hand.position[0] is not None:\n",
    "        cv2.putText(debug, '{:.2f},{:.2f}'.format(hand.velocity[0], hand.velocity[1]), (0, 50), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0))\n",
    "        cv2.circle(debug, (int(hand.position[0]), int(hand.position[1])), 4, (255,0,0), -1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ''' KEY POINTS/REGIONS '''\n",
    "    if hand_contour is not None:\n",
    "        most_left  = tuple(hand_contour[hand_contour[:, :, 0].argmin()][0])\n",
    "        most_right = tuple(hand_contour[hand_contour[:, :, 0].argmax()][0])\n",
    "        most_down  = tuple(hand_contour[hand_contour[:, :, 1].argmax()][0])\n",
    "\n",
    "        eoh = most_left if walks_left else most_right # end of hand\n",
    "        tl = (int(eoh[0]), int(hand.position[1]))\n",
    "        br = (int(hand.position[0]), h-1)\n",
    "        legs_roi = (tl, br) \n",
    "\n",
    "        # Debug\n",
    "        cv2.circle(debug, most_left, 4, (255,0,255), -1)\n",
    "        cv2.circle(debug, most_down, 4, (0,255,0), -1)\n",
    "        cv2.rectangle(debug, tl, br, (0,0,255), 2)\n",
    "\n",
    "    \n",
    "    \n",
    "    ''' EDGES '''\n",
    "#     edges = cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY)\n",
    "#     edges = cv2.adaptiveThreshold(edges, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "#     canny = cv2.Canny(blurred, 10, 10)\n",
    "    \n",
    "    \n",
    "        \n",
    "    ''' OPTICAL FLOW '''\n",
    "    curr = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, curr, None, **fb_params)\n",
    "    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "    \n",
    "    p0 = legs_roi[0]\n",
    "    p1 = legs_roi[1]\n",
    "    legs_mag = mag[p0[1]:p1[1], p0[0]:p1[0]]#*np.abs(np.cos(ang[p0[1]:p1[1], p0[0]:p1[0]]))\n",
    "    hand.leg_speed = np.mean(legs_mag)\n",
    "    \n",
    "    cv2.putText(debug, '{:.2f}'.format(np.mean(legs_mag)), (0, 30), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0))\n",
    "    \n",
    "    # Ignore non-hand movement\n",
    "    #mag = cv2.bitwise_and(mag, mag, mask=hand_mask)    \n",
    "       \n",
    "        \n",
    "        \n",
    "    ''' MHI '''\n",
    "#     mag_hist = mhi_alpha*mag_hist + (1-mhi_alpha)*mag\n",
    "#     ang_hist = mhi_alpha*ang_hist + (1-mhi_alpha)*ang\n",
    "#     mhi[...,0] = ang_hist*180/np.pi/2\n",
    "#     mhi[...,2] = cv2.normalize(mag_hist, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ''' GESTURES '''\n",
    "    hand.check_cooldowns()\n",
    "    cv2.putText(debug, hand.gestures_pretty() + hand.r_direction.value, (0, h), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0))\n",
    "    \n",
    "    ''' INPUT '''\n",
    "    if input_toggle:\n",
    "        if Gesture.JUMP in hand.gestures:\n",
    "            GameInput.jump()\n",
    "        else:\n",
    "            GameInput.stop_jump()\n",
    "\n",
    "        if Gesture.RUN in hand.gestures:\n",
    "            GameInput.walk(hand.r_direction)\n",
    "        else:\n",
    "            GameInput.stop_move()\n",
    "    \n",
    "    ''' OUTPUT/DEBUG '''\n",
    "    hand_mask = cv2.cvtColor(hand_mask, cv2.COLOR_GRAY2BGR)\n",
    "#     edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "#     canny = cv2.cvtColor(canny, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # Create color representation of optical flow\n",
    "    flow_vis[...,0] = ang*180/np.pi/2\n",
    "    flow_vis[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    flow_vis_bgr = cv2.cvtColor(flow_vis,cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    # Create color representation of mhi\n",
    "    mhi_bgr = cv2.cvtColor(mhi, cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    cv2.imshow('frame',np.hstack((debug, cv2.cvtColor(skin_mask, cv2.COLOR_GRAY2BGR), hand_mask, flow_vis_bgr))) #flow_vis_bgr, mhi_bgr)))\n",
    "\n",
    "    # Exit on ESC\n",
    "    key = cv2.waitKey(3) & 0xFF\n",
    "    if key == 27:\n",
    "        break\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    if key == ord(' '):\n",
    "        input_toggle = not input_toggle\n",
    "    if key == ord('s'):\n",
    "        cv2.imwrite('{}.png'.format(str(time.time())), np.hstack((debug, cv2.cvtColor(skin_mask, cv2.COLOR_GRAY2BGR), hand_mask, flow_vis_bgr)))\n",
    "\n",
    "    prvs = curr\n",
    "        \n",
    "cap.kill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of emergency\n",
    "cap.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
