{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finger People experimental playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import urllib\n",
    "import time\n",
    "from enum import Enum\n",
    "import subprocess\n",
    "import keras.models\n",
    "import pickle\n",
    "\n",
    "from helpers import imshow\n",
    "from capture import Capture, CapType\n",
    "from segmenter import *\n",
    "from gestures import *\n",
    "from game_input import GameInput\n",
    "import vision\n",
    "import dataset\n",
    "import learn\n",
    "from cnn_input import CnnInput\n",
    "import preprocess\n",
    "import finger_people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IP Webcam\n",
    "cap_source = 'http://192.168.0.124:8080/video'\n",
    "cap_type = CapType.VIDEO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Data\n",
    "Record dense data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_name = 'run'\n",
    "max_frames = 300\n",
    "n_frames = 0\n",
    "output_dir = '../data/'\n",
    "\n",
    "output_path = os.path.join(output_dir,\n",
    "        '{}-{}.avi'.format(gesture_name, str(time.time())))\n",
    "\n",
    "cap = Capture(cap_source, cap_type)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_path, fourcc, 30.0, (160,120))\n",
    "\n",
    "while cap.is_opened() and n_frames < max_frames:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "        \n",
    "    out.write(frame)\n",
    "    n_frames += 1\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    key = cv2.waitKey(3) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "            \n",
    "cap.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record sparse data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_name = 'kick'\n",
    "max_frames = 300\n",
    "n_frames = 0\n",
    "recorded_frames = set()\n",
    "output_dir = '../data/'\n",
    "output_prefix = os.path.join(output_dir,\n",
    "        '{}-{}'.format(gesture_name, str(time.time())))\n",
    "output_path = output_prefix + '.avi'\n",
    "output_pickle = output_prefix + '.pickle'\n",
    "\n",
    "cap = Capture(cap_source, cap_type)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_path, fourcc, 30.0, (160,120))\n",
    "flow = vision.OpticalFlow(cap.read()[1])\n",
    "record = False\n",
    "\n",
    "while cap.is_opened() and len(recorded_frames) < max_frames:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    \n",
    "    out.write(frame)\n",
    "    flow.update(frame)\n",
    "    \n",
    "    if record:\n",
    "        recorded_frames.add(n_frames)        \n",
    "        cv2.circle(frame, (6,6), (5), (0,0,255), cv2.FILLED)\n",
    "    n_frames += 1\n",
    "        \n",
    "    cv2.imshow('frame', np.hstack((frame, flow.vis)))\n",
    "    \n",
    "    key = cv2.waitKey(3) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    if key == ord('f'):\n",
    "        record = not record\n",
    "            \n",
    "pickle.dump(recorded_frames, open(output_pickle, 'wb'))\n",
    "cap.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b8bebcf2104d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "preprocess.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '4_long_train.hdf5'\n",
    "model_dir = 'models'\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test how fast our model runs while also doing a basic sanity check for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/preprocessed/walk/{}.png'\n",
    "predictions = np.zeros(dataset.n_classes)\n",
    "start = time.time()\n",
    "n = 600\n",
    "for i in range(n):\n",
    "    frame = cv2.imread(path.format(i))\n",
    "    frame = np.expand_dims(frame, 0)\n",
    "    prediction = model.predict(frame)\n",
    "    label_id = np.argmax(prediction)\n",
    "    predictions[label_id] = predictions[label_id] + 1\n",
    "\n",
    "label_predictions = {}\n",
    "for i, prediction in enumerate(predictions):\n",
    "    label_predictions[dataset.id_to_gesture[i]] = predictions[i]\n",
    "print(label_predictions)\n",
    "\n",
    "rate = n / (time.time() - start)\n",
    "print('{:.2f} forward passes per second.'.format(rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finger People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finger_people.finger_people(model_path, cap_source, cap_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of emergency\n",
    "cap.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skin Clibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUE = 0\n",
    "SAT = 1\n",
    "VAL = 2\n",
    "channels = [HUE, SAT]\n",
    "\n",
    "HUE_RANGE = (0,180)\n",
    "SAT_RANGE = (0,256)\n",
    "VAL_RANGE = (0, 256)\n",
    "ranges = [*HUE_RANGE, *SAT_RANGE]\n",
    "\n",
    "num_bins = 32\n",
    "bins = [num_bins]*len(channels)\n",
    "\n",
    "sat_thresh = 16 # any saturations below will be thrown out due to instability\n",
    "sat_thresh_bin = int(sat_thresh/SAT_RANGE[1]*num_bins)\n",
    "\n",
    "# Get samples of skin\n",
    "cap = Capture(cap_source, cap_type)\n",
    "skin_samples = []\n",
    "for _ in range(5):\n",
    "    skin_samples.append(cv2.cvtColor(get_roi_sample(cap), cv2.COLOR_BGR2HSV))\n",
    "cap.kill()\n",
    "\n",
    "# Calculate histogram\n",
    "skin_hist = mean_hist(skin_samples, channels=channels, ranges=ranges, bins=bins)\n",
    "#skin_hist[:, :sat_thresh_bin] = 0\n",
    "plt.xlabel('Saturation')\n",
    "plt.ylabel('Hue')\n",
    "plt.title('Your Skin')\n",
    "plt.imshow(skin_hist)\n",
    "\n",
    "# Create function to mask skin\n",
    "mask_skin = lambda frame, thresh=1: hist_mask(frame, skin_hist, thresh=thresh, channels=channels, ranges=ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finger People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = Capture(cap_source, cap_type)\n",
    "\n",
    "hand = Hand()\n",
    "\n",
    "walks_left = True # The direction the finger person is facing in the (mirrored) frame\n",
    "if not walks_left:\n",
    "    hand.body_facing = Direction.RIGHT\n",
    "    \n",
    "input_toggle = False\n",
    "\n",
    "min_blob_size = 0\n",
    "mhi_alpha = 0.5\n",
    "\n",
    "# Take the first frame\n",
    "_ret, init_frame = cap.read()\n",
    "prvs = cv2.cvtColor(init_frame,cv2.COLOR_BGR2GRAY)\n",
    "frame1 = cv2.flip(init_frame, 1)\n",
    "\n",
    "h = init_frame.shape[0]\n",
    "w = init_frame.shape[1]\n",
    "hand.screen_width = w\n",
    "\n",
    "# For color representation of optical flow\n",
    "flow_vis = np.zeros_like(init_frame)\n",
    "flow_vis[...,1] = 255\n",
    "\n",
    "# Motion history image\n",
    "mag_hist = np.zeros((init_frame.shape[:2]), dtype=np.float32)\n",
    "ang_hist = np.zeros((init_frame.shape[:2]), dtype=np.float32)\n",
    "mhi = np.zeros_like(init_frame)\n",
    "mhi[...,1] = 255\n",
    "\n",
    "while cap.is_opened():\n",
    "    \n",
    "    # Get the next frame\n",
    "    _ret, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)  \n",
    "    if not _ret:\n",
    "        break\n",
    "    debug = frame.copy()\n",
    "      \n",
    "    \n",
    "    \n",
    "    ''' SKIN MASK '''\n",
    "    blurred = cv2.GaussianBlur(frame, (7,7), 0)\n",
    "    skin_mask = mask_skin(cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV), thresh=25)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    hand_mask = skin_mask\n",
    "    \n",
    "    hand_mask = cv2.morphologyEx(hand_mask, cv2.MORPH_OPEN, kernel, iterations = 1)\n",
    "    hand_mask = cv2.morphologyEx(hand_mask, cv2.MORPH_CLOSE, kernel, iterations = 2)\n",
    "    hand_contour = largest_blob(hand_mask, thresh=min_blob_size)\n",
    "    hand_mask = contour2mask(hand_contour, shape=skin_mask)\n",
    "    \n",
    "    hand.position = contour_pos(hand_contour)\n",
    "    \n",
    "    # debug\n",
    "    if hand.position[0] is not None:\n",
    "        cv2.putText(debug, '{:.2f},{:.2f}'.format(hand.velocity[0], hand.velocity[1]), (0, 50), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0))\n",
    "        cv2.circle(debug, (int(hand.position[0]), int(hand.position[1])), 4, (255,0,0), -1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ''' KEY POINTS/REGIONS '''\n",
    "    if hand_contour is not None:\n",
    "        most_left  = tuple(hand_contour[hand_contour[:, :, 0].argmin()][0])\n",
    "        most_right = tuple(hand_contour[hand_contour[:, :, 0].argmax()][0])\n",
    "        most_down  = tuple(hand_contour[hand_contour[:, :, 1].argmax()][0])\n",
    "\n",
    "        eoh = most_left if walks_left else most_right # end of hand\n",
    "        tl = (int(eoh[0]), int(hand.position[1]))\n",
    "        br = (int(hand.position[0]), h-1)\n",
    "        legs_roi = (tl, br) \n",
    "\n",
    "        # Debug\n",
    "        cv2.circle(debug, most_left, 4, (255,0,255), -1)\n",
    "        cv2.circle(debug, most_down, 4, (0,255,0), -1)\n",
    "        cv2.rectangle(debug, tl, br, (0,0,255), 2)\n",
    "\n",
    "    \n",
    "    \n",
    "    ''' EDGES '''\n",
    "#     edges = cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY)\n",
    "#     edges = cv2.adaptiveThreshold(edges, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "#     canny = cv2.Canny(blurred, 10, 10)\n",
    "    \n",
    "    \n",
    "        \n",
    "    ''' OPTICAL FLOW '''\n",
    "    curr = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, curr, None, **fb_params)\n",
    "    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "    \n",
    "    p0 = legs_roi[0]\n",
    "    p1 = legs_roi[1]\n",
    "    legs_mag = mag[p0[1]:p1[1], p0[0]:p1[0]]#*np.abs(np.cos(ang[p0[1]:p1[1], p0[0]:p1[0]]))\n",
    "    hand.leg_speed = np.mean(legs_mag)\n",
    "    \n",
    "    cv2.putText(debug, '{:.2f}'.format(np.mean(legs_mag)), (0, 30), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0))\n",
    "    \n",
    "    # Ignore non-hand movement\n",
    "    #mag = cv2.bitwise_and(mag, mag, mask=hand_mask)    \n",
    "       \n",
    "        \n",
    "        \n",
    "    ''' MHI '''\n",
    "#     mag_hist = mhi_alpha*mag_hist + (1-mhi_alpha)*mag\n",
    "#     ang_hist = mhi_alpha*ang_hist + (1-mhi_alpha)*ang\n",
    "#     mhi[...,0] = ang_hist*180/np.pi/2\n",
    "#     mhi[...,2] = cv2.normalize(mag_hist, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ''' GESTURES '''\n",
    "    hand.check_cooldowns()\n",
    "    cv2.putText(debug, hand.gestures_pretty() + hand.r_direction.value, (0, h), cv2.FONT_HERSHEY_DUPLEX, 0.5, (0,255,0))\n",
    "    \n",
    "    ''' INPUT '''\n",
    "    if input_toggle:\n",
    "        if Gesture.JUMP in hand.gestures:\n",
    "            GameInput.jump()\n",
    "        else:\n",
    "            GameInput.stop_jump()\n",
    "\n",
    "        if Gesture.RUN in hand.gestures:\n",
    "            GameInput.walk(hand.r_direction)\n",
    "        else:\n",
    "            GameInput.stop_move()\n",
    "    \n",
    "    ''' OUTPUT/DEBUG '''\n",
    "    hand_mask = cv2.cvtColor(hand_mask, cv2.COLOR_GRAY2BGR)\n",
    "#     edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "#     canny = cv2.cvtColor(canny, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # Create color representation of optical flow\n",
    "    flow_vis[...,0] = ang*180/np.pi/2\n",
    "    flow_vis[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    flow_vis_bgr = cv2.cvtColor(flow_vis,cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    # Create color representation of mhi\n",
    "    mhi_bgr = cv2.cvtColor(mhi, cv2.COLOR_HSV2BGR)\n",
    "    \n",
    "    cv2.imshow('frame',np.hstack((debug, cv2.cvtColor(skin_mask, cv2.COLOR_GRAY2BGR), hand_mask, flow_vis_bgr))) #flow_vis_bgr, mhi_bgr)))\n",
    "\n",
    "    # Exit on ESC\n",
    "    key = cv2.waitKey(3) & 0xFF\n",
    "    if key == 27:\n",
    "        break\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    if key == ord(' '):\n",
    "        input_toggle = not input_toggle\n",
    "    if key == ord('s'):\n",
    "        cv2.imwrite('{}.png'.format(str(time.time())), np.hstack((debug, cv2.cvtColor(skin_mask, cv2.COLOR_GRAY2BGR), hand_mask, flow_vis_bgr)))\n",
    "\n",
    "    prvs = curr\n",
    "        \n",
    "cap.kill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of emergency\n",
    "cap.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
